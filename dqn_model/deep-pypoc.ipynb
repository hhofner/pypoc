{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1546847731  # or try a new seed by using: seed = int(time())\n",
    "random.seed(seed)\n",
    "print('Seed: {}'.format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPyPocNetwork:\n",
    "    def __init__(self, hidden_layers_size, gamma, learning_rate, input_size=4, output_size=4):\n",
    "        self.q_target = tf.placeholder(shape=(None,output_size), dtype=tf.float32)\n",
    "        self.r = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "        self.states = tf.placeholder(shape=(None, input_size), dtype=tf.float32)\n",
    "        self.enum_actions = tf.placeholder(shape=(None,2), dtype=tf.int32) \n",
    "        layer = self.states\n",
    "        for l in hidden_layers_size:\n",
    "            layer = tf.layers.dense(inputs=layer, units=l, activation=tf.nn.relu,\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "        self.output = tf.layers.dense(inputs=layer, units=output_size,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(seed=seed))\n",
    "        self.predictions = tf.gather_nd(self.output,indices=self.enum_actions)\n",
    "        self.labels = self.r + gamma * tf.reduce_max(self.q_target, axis=1)\n",
    "        self.cost = tf.reduce_mean(tf.losses.mean_squared_error(labels=self.labels, predictions=self.predictions))\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    memory = None\n",
    "    counter = 0\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque(maxlen=size)\n",
    "\n",
    "    def append(self, element):\n",
    "        self.memory.append(element)\n",
    "        self.counter += 1\n",
    "\n",
    "    def sample(self, n):\n",
    "        return random.sample(self.memory, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_games = 200\n",
    "epsilon = 0.1\n",
    "gamma = 0.99\n",
    "batch_size = 10\n",
    "memory_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(seed)\n",
    "qnn = QPyPocNetwork(hidden_layers_size=[20,20], gamma=gamma, learning_rate=0.001)\n",
    "memory = ReplayMemory(memory_size)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_list = []  \n",
    "c_list = []  # same as r_list, but for the cost\n",
    "\n",
    "counter = 0  # will be used to trigger network training\n",
    "\n",
    "#TODO: Create a network object here\n",
    "\n",
    "while(network_running):\n",
    "    counter += 1\n",
    "    state = np.copy(node_of_interest.neighbor_state)\n",
    "\n",
    "for g in range(num_of_games):\n",
    "    simulation_over = False\n",
    "    simulation.reset()\n",
    "    total_reward = 0\n",
    "    while not simulation_over:\n",
    "        counter += 1\n",
    "        state = np.copy(node_of_interest.neighbor_state)\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randint(0,3)\n",
    "        else:\n",
    "            pred = np.squeeze(sess.run(qnn.output,feed_dict={qnn.states: np.expand_dims(game.board,axis=0)}))\n",
    "            action = np.argmax(pred)\n",
    "        reward, simulation_over = game.play(action)\n",
    "        total_reward += reward\n",
    "        next_state = np.copy(game.board)\n",
    "        memory.append({'state':state,'action':action,'reward':reward,'next_state':next_state,'game_over':game_over})\n",
    "        if counter % batch_size == 0:\n",
    "            # Network training\n",
    "            batch = memory.sample(batch_size)\n",
    "            q_target = sess.run(qnn.output,feed_dict={qnn.states: np.array(list(map(lambda x: x['next_state'], batch)))})\n",
    "            terminals = np.array(list(map(lambda x: x['game_over'], batch)))\n",
    "            for i in range(terminals.size):\n",
    "                if terminals[i]:\n",
    "                    # Remember we use the network's own predictions for the next state while calculatng loss.\n",
    "                    # Terminal states have no Q-value, and so we manually set them to 0, as the network's predictions\n",
    "                    # for these states is meaningless\n",
    "                    q_target[i] = np.zeros(game.board_size)\n",
    "            _, cost = sess.run([qnn.optimizer, qnn.cost], \n",
    "                               feed_dict={qnn.states: np.array(list(map(lambda x: x['state'], batch))),\n",
    "                               qnn.r: np.array(list(map(lambda x: x['reward'], batch))),\n",
    "                               qnn.enum_actions: np.array(list(enumerate(map(lambda x: x['action'], batch)))),\n",
    "                               qnn.q_target: q_target})\n",
    "            c_list.append(cost)\n",
    "    r_list.append(total_reward)\n",
    "print('Final cost: {}'.format(c_list[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
